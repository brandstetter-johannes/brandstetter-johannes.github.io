
@InProceedings{pmlr-v162-brandstetter22a,
  title = 	 {Lie Point Symmetry Data Augmentation for Neural {PDE} Solvers},
  author =       {Brandstetter, Johannes and Welling, Max and Worrall, Daniel E},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {2241--2256},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/brandstetter22a/brandstetter22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/brandstetter22a.html},
  abstract = 	 {Neural networks are increasingly being used to solve partial differential equations (PDEs), replacing slower numerical solvers. However, a critical issue is that neural PDE solvers require high-quality ground truth data, which usually must come from the very solvers they are designed to replace. Thus, we are presented with a proverbial chicken-and-egg problem. In this paper, we present a method, which can partially alleviate this problem, by improving neural PDE solver sample complexityâ€”Lie point symmetry data augmentation (LPSDA). In the context of PDEs, it turns out we are able to quantitatively derive an exhaustive list of data transformations, based on the Lie point symmetry group of the PDEs in question, something not possible in other application areas. We present this framework and demonstrate how it can easily be deployed to improve neural PDE solver sample complexity by an order of magnitude.}
}
