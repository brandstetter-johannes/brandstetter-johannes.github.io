[{"authors":null,"categories":null,"content":"I am leading a group \u0026ldquo;AI for data-driven simulations\u0026rdquo; at the Institute for Machine Learning at the Johannes Kepler University (JKU) Linz. Additionally, I am a VP Research at NXAI - our new European AI hub in Linz (Austria).\nI have obtained my PhD after working several years at the CMS experiment at CERN. During this time, I had the privilege of learning from brilliant minds from all around the world, and got the chance to co-author seminal papers in the realm of Higgs boson physics. In 2018, after completing my PhD, my career trajectory shifted towards machine learning, and I was fortunate to join the research group of Mr LSTM Sepp Hochreiter in Linz. Under Sepp\u0026rsquo;s mentorship, I delved into the intricacies of machine learning and modern deep learning over a span of 2.5 years.\nFrom 2021 to 2023, I had the pleasure of spending three remarkable years in Amsterdam. Initially, I was part of the Amsterdam Machine Learning Lab lead by Max Welling, and subsequently joined Microsoft Research for 2 years. During this period, my passion for Geometric Deep Learning, particularly involving Geometric (Clifford) algebras, and my interest in partial differential equations (PDEs), with a particular focus on developing neural surrogates for (PDEs), became profound. Most importantly, I pivoted towards large-scale PDEs, including weather and climate modeling, which culminated in Aurora.\nMy years in Amsterdam have shaped my research vision. I am firmly convinced that AI is on the cusp of disrupting simulations at industry-scale. Every day thousands and thousands of compute hours are spent on turbulence modeling, simulations of fluid or air flows, heat transfer in materials, traffic flows, and many more. Many of these processes follow similar underlying patterns, but yet need different and extremely specialized softward to simulate. Even worse, for different parameter settings the costly simulations need to be run at full length from scratch.\nThis is what I want to change! Therefore, I have started a new group at JKU Linz which has strong computer vision, numerical simulation, and engineering components. We want to advance data-driven simulations at industry-scale, and place the Austrian industry engine Linz as a center for doing that.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am leading a group \u0026ldquo;AI for data-driven simulations\u0026rdquo; at the Institute for Machine Learning at the Johannes Kepler University (JKU) Linz. Additionally, I am a VP Research at NXAI - our new European AI hub in Linz (Austria).","tags":null,"title":"Johannes Brandstetter","type":"authors"},{"authors":["Benedikt Alkin","Maximilian Beck","Korbinian Poeppel","Sepp Hochreiter","Johannes Brandstetter"],"categories":["Computer Vision"],"content":"","date":1717671317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717671317,"objectID":"d97fca4898494325db1c24270a4d6066","permalink":"https://brandstetter-johannes.github.io/publication/alkin-2024-vision-lstm/","publishdate":"2024-06-06T12:55:17+02:00","relpermalink":"/publication/alkin-2024-vision-lstm/","section":"publication","summary":"We introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision.","tags":["Computer Vision","xLSTM","Pretraining","Deep Learning"],"title":"Vision-LSTM -- xLSTM as Generic Vision Backbone","type":"publication"},{"authors":["Benedikt Alkin","Maximilian Beck","Korbinian Poeppel","Sepp Hochreiter","Johannes Brandstetter"],"categories":["Computer Vision"],"content":"","date":1717671317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717671317,"objectID":"5519c1b90913b1048f86118abd3147ed","permalink":"https://brandstetter-johannes.github.io/publication/beck-2024-xlstm/","publishdate":"2024-06-06T12:55:17+02:00","relpermalink":"/publication/beck-2024-xlstm/","section":"publication","summary":"We introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision.","tags":["Computer Vision","xLSTM","Pretraining","Deep Learning"],"title":"Vision-LSTM -- xLSTM as Generic Vision Backbone","type":"publication"},{"authors":["Benedikt Alkin","Andreas Fürst","Simon Schmid","Lukas Gruber","Markus Holzleitner","Johannes Brandstetter"],"categories":["Learn2Simulate"],"content":"","date":1708340117,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708340117,"objectID":"5b6a058575d4eff3042258460db14057","permalink":"https://brandstetter-johannes.github.io/publication/alkin-2024-upt/","publishdate":"2022-02-07T12:55:17+02:00","relpermalink":"/publication/alkin-2024-upt/","section":"publication","summary":"We introduce Universal Physics Transformers (UPTs), an efficient and unified learning paradigm for a wide range of spatio-temporal problems. UPTs operate without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles.","tags":["Partial Differential Equations","AI4Science","Neural Surrogates","Neural Operators","Learn2Simulate","Deep Learning"],"title":"Universal Physics Transformers -- A Framework For Efficiently Scaling Neural Operators","type":"publication"},{"authors":["Benedikt Alkin","Lukas Miklautz","Sepp Hochreiter","Johannes Brandstetter"],"categories":["Computer Vision"],"content":"","date":1707994517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707994517,"objectID":"b9771bf0b4f8c48c7c837a51a6c01100","permalink":"https://brandstetter-johannes.github.io/publication/alkin-2024-mimrefiner/","publishdate":"2024-02-15T12:55:17+02:00","relpermalink":"/publication/alkin-2024-mimrefiner/","section":"publication","summary":"We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models.","tags":["Computer Vision","Masked Image Modeling","Pretraining","Deep Learning"],"title":"Mim-refiner -- A contrastive learning boost from intermediate pre-trained representations","type":"publication"},{"authors":["Tara Akhound-Sadegh","Laurence Perreault-Levasseur","Johannes Brandstetter","Max Welling","Siamak Ravanbakhsh"],"categories":["Lie Point Symmetries"],"content":"","date":1699354517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699354517,"objectID":"d9b070b4b5276a2a45ea39f578d6f77b","permalink":"https://brandstetter-johannes.github.io/publication/akhound-sadegh-2023-lps-pinn/","publishdate":"2023-11-07T12:55:17+02:00","relpermalink":"/publication/akhound-sadegh-2023-lps-pinn/","section":"publication","summary":"We present how to use Lie Point Symmetries of PDEs to improve physics-informed neural networks. Published at NeurIPS 2023.","tags":["Partial Differential Equations","PINNs","Neural Surrogates","Lie Point Symmetries","Geometric Deep Learning","Deep Learning"],"title":"Lie Point Symmetries and Physics-Informed Networks","type":"publication"},{"authors":null,"categories":null,"content":"My years in Amsterdam \u0026ndash; first at the Amsterdam Machine Learning Lab and then 2 years at in the AI4Science lab at Microsoft Research \u0026ndash; have shaped my research vision. I am firmly convinced that AI is on the cusp of disrupting simulations at industry-scale. Therefore, I have started a new group at JKU Linz which has strong computer vision, numerical simulation, and engineering components. We want to advance data-driven simulations at industry-scale, and place the Austrian industry engine Linz as a center for doing that.\nAbout my research Working in high energy physics means to make the invisible visible. When we published for example the first direct observation of the Higgs boson decaying into pairs of tau leptons non of the processes, i.e., the Higgs boson signal itself nor the many \u0026lsquo;\u0026lsquo;background\u0026rsquo;\u0026rsquo; processes, was measured directly. Everything needed to be reconstructed from terabytes of detector data, and \u0026ndash; even more fascinating to me \u0026ndash; by using sophisticated physical simulations.\nMy first contact with simulations in the context of Deep Learning was through the work on Boundary graph neural networks for 3D simulations, which was financed by the Austrian Research Promotion Agency (FFG) after winning a research grant together with DCS computing.\nDuring my years in Amsterdam I fully pivoted towards neural modeling of partial differential equations (PDEs). For example:\n In Message Passing Neural PDE Solvers, we replaced all heuristically designed components in numerical PDE solvers with backprop-optimized neural function approximators. We studied how to use Lie Point Symmetries of PDEs to improve sample complexity of neural PDE solvers. In Clifford Neural Layers for PDE Modeling and Geometric Clifford Algebra Networks, we introduced neural network layers for PDE modeling, which are based on operations on composite objects of scalars, vectors, and higher order objects. In Towards Multi-spatiotemporal-scale Generalized PDE Modeling, we introduce PDEArena, a modern PyTorch Lightning-based deep learning framework for neural PDE modeling with a plethora of state-of-the art neural PDE surrogates and various temporal PDEs in 1-, 2-, and 3-spatial dimensions. In ClimaX, we developed a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets. ClimaX is the first foundation model for weather and climate.  How to disrupt simulations at industry-scale Recent years have shown that scalable ideas, improving the datasets, and clever engineering are the ingredients for ever better Deep Learning systems. LLMs and large-scale computer vision models have given birth to the heavily overused term \u0026ldquo;foundation model\u0026rdquo;. After language and computer vision, foundation models have spread over to various disciplines, with AlphaFold as very prominent example.\nI want to go one step further, and bring foundation models to the every-day engineering world, and in doing so, disrupt how our industry works. Every day thousands and thousands of compute hours are spent on turbulence modeling, simulations of fluid or air flows, heat transfer in materials, traffic flows, and many more. Many of these processes follow similar underlying patterns, but yet need different and extremely specialized softward to simulate. Even worse, for different parameter settings the costly simulations need to be run at full length from scratch.\nThis is what I want to change! Deep Learning techniques are ready to develope models which run simulations in seconds instead of days or even weeks. The hardware is ready to digest high-resolutional industry-scale inputs, e.g., 3D meshes or images, and subsequently sets the stage to train Deep Learning models at scale.\nBut what about the data you might ask? Don\u0026rsquo;t we need high-quality ground truth data, which usually must come from the very simulations we are going to replace, presenting us with a proverbial chicken-and-egg problem? Luckily, many of the above mentioned processes have common underlying dynamics \u0026ndash; similar to how different languages share structure and grammar. Simulation data is abundant, we just need to use the right ones, and many of that.\nPersonally, I have experienced modern computer vision as one of the driving tools for developing data-driven simulations. But it will take much more. It will need expertise in numerical simulations, geometric and physics-informed deep learning, and engineering at scale. We have already put together a very strong interdisciplinary team with different expertise, and keep hiring. The journey is going to be exciting.\nStay stuned, first works will appear soon ","date":1696118400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696118400,"objectID":"2fd9e67842e8975b7bf6578bf0ece1fe","permalink":"https://brandstetter-johannes.github.io/research/data-driven-simulations/","publishdate":"2023-10-01T00:00:00Z","relpermalink":"/research/data-driven-simulations/","section":"research","summary":"I am firmly convinced that AI is on the cusp of disrupting simulations at industry-scale. Therefore, I have started a new group at JKU Linz which has strong computer vision, simulation, and engineering components. My vision is shaped by experience both from university and from industry.","tags":["Learning2Simulate","AI4Science","AI4Engineering","Partial Differential Equations","Neural Solvers","Geometric Deep Learning","Deep Learning"],"title":"Data-Driven Simulations","type":"research"},{"authors":["Phillip Lippe","Bastiaan S. Veeling","Paris Perdikaris","Richard E. Turner","Johannes Brandstetter"],"categories":["Learn2Simulate"],"content":"","date":1691664917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691664917,"objectID":"ec3a9a5561730d72529a5b4e5e946ed3","permalink":"https://brandstetter-johannes.github.io/publication/lippe-2023-pderefiner/","publishdate":"2023-08-10T12:55:17+02:00","relpermalink":"/publication/lippe-2023-pderefiner/","section":"publication","summary":"PDE-Refiner is an iterative refinement process that enables neural operator training for accurate and stable predictions over long time horizons. Published at NeurIPS 2023 (Spotlight).","tags":["Partial Differential Equations","Learn2Simulate","AI4Science","Neural Surrogates","Neural Operators","Microsoft","Deep Learning"],"title":"PDE-Refiner - Achieving Accurate Long Rollouts with Neural PDE Solvers","type":"publication"},{"authors":["Artur P. Toshev","Gianluca Galletti","Johannes Brandstetter","Stefan Adami","Nikolaus A. Adams"],"categories":["Learn2Simulate"],"content":"","date":1684925717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684925717,"objectID":"0e89d1de236a0146a08cf1d9bc498303","permalink":"https://brandstetter-johannes.github.io/publication/toshev-2023-lagrangian/","publishdate":"2023-05-24T12:55:17+02:00","relpermalink":"/publication/toshev-2023-lagrangian/","section":"publication","summary":"We introduce E(3)-equivariant GNNs to two well-studied fluid-flow systems, namely 3D decaying Taylor-Green vortex and 3D reverse Poiseuille flow. Published at GSI 2023.","tags":["Partial Differential Equations","Learn2Simulate","Geometric Deep Learning","AI4Science","Graph Neural Networks","Equivariance","Neural Surrogates","Lagrangian Fluid Mechanics","Deep Learning"],"title":"Learning Lagrangian Fluid Mechanics with E(3)-Equivariant Graph Neural Networks","type":"publication"},{"authors":["David Ruhe","Johannes Brandstetter","Patrick Forré"],"categories":["Geometric Deep Learning"],"content":"","date":1684407317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684407317,"objectID":"08f8f02bc5df0c67799dd6a0a1c0fef1","permalink":"https://brandstetter-johannes.github.io/publication/ruhe-2023-cgenns/","publishdate":"2023-05-18T12:55:17+02:00","relpermalink":"/publication/ruhe-2023-cgenns/","section":"publication","summary":"We introduce a novel method to construct E(n)- and O(n)-equivariant neural networks using Clifford algebras. Published at NeurIPS 2023 (Oral).","tags":["Clifford Algebras","Geometric Algebras","Geometric Deep Learning","AI4Science","Graph Neural Networks","Equivariance","Deep Learning"],"title":"Clifford Group Equivariant Neural Networks","type":"publication"},{"authors":["David Ruhe","Jayesh K. Gupta","Steven de Keninck","Max Welling","Johannes Brandstetter"],"categories":["Geometric Deep Learning"],"content":"","date":1676285717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1676285717,"objectID":"837e07a343ece28af3f719348152c942","permalink":"https://brandstetter-johannes.github.io/publication/ruhe-2023-cgans/","publishdate":"2023-02-13T12:55:17+02:00","relpermalink":"/publication/ruhe-2023-cgans/","section":"publication","summary":"We introduce Geometric Clifford Algebra Networks (GCANs) which parameterize combinations of learnable group actions. Published at ICML 2023.","tags":["Partial Differential Equations","Learn2Simulate","Neural Surrogates","Clifford Algebras","Geometric Algebras","Geometric Deep Learning","AI4Science","Neural Solvers","Microsoft","Graph Neural Networks","Deep Learning"],"title":"Geometric Clifford Algebra Networks","type":"publication"},{"authors":["Tung Nguyen","Johannes Brandstetter","Ashish Kapoor","Jayesh K. Gupta","Aditya Grover"],"categories":["Learn2Simulate"],"content":"","date":1674518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674550076,"objectID":"4f3b98f939cc107be3871b4ed10b4ca3","permalink":"https://brandstetter-johannes.github.io/publication/bodnar-2024-aurora/","publishdate":"2023-01-24T08:47:56.212636Z","relpermalink":"/publication/bodnar-2024-aurora/","section":"publication","summary":"We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. Published at ICML 2023 (Spotlight).","tags":["Partial Differential Equations","Learn2Simulate","Weather\u0026Climate","AI4Science","Neural Surrogates","Microsoft","Deep Learning"],"title":"ClimaX -- A foundation model for weather and climate","type":"publication"},{"authors":["Tung Nguyen","Johannes Brandstetter","Ashish Kapoor","Jayesh K. Gupta","Aditya Grover"],"categories":["Learn2Simulate"],"content":"","date":1674518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1674550076,"objectID":"cbf8e9e901bd1ee3fb815ba2cc2f275d","permalink":"https://brandstetter-johannes.github.io/publication/nguyen-2023-climax/","publishdate":"2023-01-24T08:47:56.212636Z","relpermalink":"/publication/nguyen-2023-climax/","section":"publication","summary":"We develop and demonstrate ClimaX, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings. Published at ICML 2023 (Spotlight).","tags":["Partial Differential Equations","Learn2Simulate","Weather\u0026Climate","AI4Science","Neural Surrogates","Microsoft","Deep Learning"],"title":"ClimaX -- A foundation model for weather and climate","type":"publication"},{"authors":["Jayesh K. Gupta","Johannes Brandstetter"],"categories":["Learn2Simulate"],"content":"","date":1664496e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664525761,"objectID":"4f399ac7dead61410543346718c691c1","permalink":"https://brandstetter-johannes.github.io/publication/gupta-2022-pdearena/","publishdate":"2022-09-30T09:16:01.817577Z","relpermalink":"/publication/gupta-2022-pdearena/","section":"publication","summary":"We present PDEArena, a modern PyTorch Lightning-based deep learning framework for neural PDE modeling. Published at TMLR 07/2023.","tags":["Partial Differential Equations","Learn2Simulate","AI4Science","Neural Surrogates","Neural Operators","Microsoft","Deep Learning"],"title":"Towards Multi-spatiotemporal-scale Generalized PDE Modeling","type":"publication"},{"authors":["Johannes Brandstetter","Rianne van den Berg","Max Welling","Jayesh Gupta"],"categories":["Geometric Deep Learning"],"content":"","date":1662634517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1662634517,"objectID":"078ef33126822109dd899b8b495234f7","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2022-clifford/","publishdate":"2022-09-08T12:55:17+02:00","relpermalink":"/publication/brandstetter-2022-clifford/","section":"publication","summary":"We introduce neural network layers based on operations on composite objects of scalars, vectors, and higher order objects such as bivectors. Published at ICLR 2023.","tags":["Partial Differential Equations","Learn2Simulate","Clifford Algebras","Geometric Algebras","Geometric Deep Learning","AI4Science","Neural Surrogates","Neural Operators","Microsoft","Deep Learning"],"title":"Clifford Neural Layers for PDE Modeling","type":"publication"},{"authors":["Johannes Brandstetter","Max Welling","Daniel E. Worrall"],"categories":["Lie Point Symmetries"],"content":"","date":1644922517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644922517,"objectID":"d6c2adb625726f807c0f08bef8d6e224","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2022-lpsda/","publishdate":"2022-02-15T12:55:17+02:00","relpermalink":"/publication/brandstetter-2022-lpsda/","section":"publication","summary":"We present how to use Lie Point Symmetries of PDEs to improve sample complexity of neural PDE solvers. Published at ICML 2022 (Spotlight).","tags":["Partial Differential Equations","Learn2Simulate","Neural Surrogates","Neural Operators","AI4Science","Geometric Deep Learning","Lie Point Symmetries","Deep Learning"],"title":"Lie Point Symmetry Data Augmentation for Neural PDE Solvers","type":"publication"},{"authors":["Johannes Brandstetter","Daniel E. Worrall","Max Welling"],"categories":["Neural PDE Solvers"],"content":"","date":1644231317,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644231317,"objectID":"70e7dd52d0c5f8f506bb560a7552347d","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2022-mpnn/","publishdate":"2022-02-07T12:55:17+02:00","relpermalink":"/publication/brandstetter-2022-mpnn/","section":"publication","summary":"In this work, we introduce a message passing neural PDE solver that replaces all heuristically designed components in numerical PDE solvers with backprop-optimized neural function approximators. Published at ICLR 2022 (Spotlight).","tags":["Partial Differential Equations","Learn2Simulate","Neural Surrogates","AI4Science","Graph Neural Networks","Deep Learning"],"title":"Message Passing Neural PDE Solvers","type":"publication"},{"authors":["Johannes Brandstetter","Rob Hesselink","Elise van der Pol","Erik Bekkers","Max Welling"],"categories":["Geometric Deep Learning"],"content":"","date":1633517717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1633517717,"objectID":"f445476f643041ca6b91bdd39e177905","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2021-segnn/","publishdate":"2021-10-06T12:55:17+02:00","relpermalink":"/publication/brandstetter-2021-segnn/","section":"publication","summary":"We generalise steerable E(3) equivariant graph neural networks such that node and edge updates are able to leverage covariant information. Published at ICLR 2022 (Spotlight).","tags":["Learn2Simulate","Geometric Deep Learning","AI4Science","Graph Neural Networks","Equivariance","Deep Learning"],"title":"Geometric and Physical Quantities Improve E(3) Equivariant Message Passing","type":"publication"},{"authors":["Andreas Mayr","Sebastian Lehner","Arno Mayrhofer","Christoph Kloss","Sepp Hochreiter","Johannes Brandstetter"],"categories":["Learn2Simulate"],"content":"","date":1624272917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624272917,"objectID":"341606f8e3221e53877cbd77717d526e","permalink":"https://brandstetter-johannes.github.io/publication/mayr-2021-bgnn/","publishdate":"2021-10-06T12:55:17+02:00","relpermalink":"/publication/mayr-2021-bgnn/","section":"publication","summary":"We generalize graph neural network based simulations of Lagrangian dynamics to complex boundaries as encountered in daily life engineering setups. Published at AAAI 2023.","tags":["Learn2Simulate","Neural Surrogates","Geometric Deep Learning","AI4Science","Graph Neural Networks","Lagrangian Fluid Mechanics","Deep Learning"],"title":"Boundary Graph Neural Networks for 3D Simulations","type":"publication"},{"authors":null,"categories":null,"content":"My passion for Geometric Deep Learning can be unmistakenly traced back to my physics background. Doing a PhD in high energy physics was confronting me with the standard model of particle physics on a daily basis. Under the hood, the standard model is a gauge quantum field theory which contains the internal symmetries of the unitary product group $SU(3) \\times SU(2) \\times U(1)$. The theory is commonly viewed as describing the fundamental set of particles \u0026ndash; leptons, quarks, gauge bosons and the Higgs boson \u0026ndash; as well as their interactions.\nUnsurprisingly, I have been deeply attracted to the field of Geometric Deep Learning which has groups, gauges, and graphs at its core. I have contributed to the fields of graph neural networks, equivariant architectures, and neural PDE solvers. Furthermore, I have lead efforts to introduce Lie Point Symmetries, and, most recently, Clifford (Geometric) Algebras into the Deep Learning community.\nFor my group \u0026ldquo;AI for data-driven simulations\u0026rdquo;, Geometric Deep Learning techniques will play an important role when working on grids, graphs, meshes, or with complex geometries in general. Recent trends on implicit parametrization, generative modeling, and latent space dynamics are very appealing as well.\nStay stuned for future work ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"28838a239a0f3fc78c164264209f5a2e","permalink":"https://brandstetter-johannes.github.io/research/geometric-deep-learning/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/research/geometric-deep-learning/","section":"research","summary":"My passion for Geometric Deep Learning can be unmistakenly traced back to my physics background. I have contributed to the fields of graph neural networks, equivariant architectures, and neural PDE solvers. Furthermore, I have lead efforts to introduce Lie Point Symmetries, and, most recently, Clifford (Geometric) Algebras into the Deep Learning community.","tags":["Geometric Deep Learning","Graph Neural Networks","Equivariance","Lie Point Symmetries","Clifford Algebras","Geometric Algebras","Deep Learning"],"title":"Geometric Deep Learning","type":"research"},{"authors":["Johannes Brandstetter","Hubert Ramsauer","Markus Holzleitner","Sepp Hochreiter","Bernhard Schäfl"],"categories":["Modern Hopfield Networks"],"content":"","date":1607770517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607770517,"objectID":"5b9e57cc6f071a1e5dd42d9b69a8950d","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2020-performer/","publishdate":"2020-12-12T12:55:17+02:00","relpermalink":"/publication/brandstetter-2020-performer/","section":"publication","summary":"Blog post which analyzes the the Performer paper from a Hopfield point of view. Published as blog post at ICLR 2022.","tags":["Modern Hopfield Networks","Self-Attention","Transformer","Associative Memory","Linear Attention","Deep Learning"],"title":"Looking at the Performer from a Hopfield point of view","type":"publication"},{"authors":["Markus Holzleitner","Lukas Gruber","José A. Arjona-Medina","Johannes Brandstetter","Sepp Hochreiter"],"categories":["Reinforcement Learning"],"content":"","date":1606906517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606906517,"objectID":"590fcda49e6650026404319c95a9faeb","permalink":"https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/","publishdate":"2020-12-02T12:55:17+02:00","relpermalink":"/publication/holzleitner-2020-convergence/","section":"publication","summary":"We prove under commonly used assumptions the convergence of actor-critic reinforcement learning algorithms. Published at Transactions on Large-Scale Data-and Knowledge-Centered Systems XLVIII.","tags":["Reinforcement Learning","Delayed Rewards","Convergence Proof","Deep Learning"],"title":"Convergence proof for actor-critic methods applied to ppo and rudder","type":"publication"},{"authors":["Vihang Patil","Markus Hofmarcher","Marius-Constantin Dinu","Matthias Dorfer","Patrick Blies","Johannes Brandstetter","José A. Arjona-Medina","Sepp Hochreiter"],"categories":["Reinforcement Learning",""],"content":"","date":1601376917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601376917,"objectID":"383697c22bccd55ddbc6d8b36b409493","permalink":"https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/","publishdate":"2020-09-29T12:55:17+02:00","relpermalink":"/publication/patil-2020-alignrudder/","section":"publication","summary":"We generalise steerable E(3) equivariant graph neural networks such that node and edge updates are able to leverage covariant information. Published at ICLR 2022 (Oral).","tags":["Reinforcement Learning","Delayed Rewards","Sequence Alignment","Deep Learning"],"title":"Align-RUDDER -- Learning From Few Demonstrations by Reward Redistribution","type":"publication"},{"authors":["Hubert Ramsauer","Bernhard Schäfl","Johannes Lehner","Philipp Seidl","Michael Widrich","Thomas Adler","Lukas Gruber","Markus Holzleitner","Milena Pavlović","Geir Kjetil Sandve","Victor Greiff","David Kreil","Michael Kopp","Günter Klambauer","Johannes Brandstetter","Sepp Hochreiter"],"categories":["Modern Hopfield Networks"],"content":"","date":1594896917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594896917,"objectID":"d217c8b0d6534e7c39b17c5c489c66a9","permalink":"https://brandstetter-johannes.github.io/publication/ramsauer-2020-hopfield/","publishdate":"2020-07-16T12:55:17+02:00","relpermalink":"/publication/ramsauer-2020-hopfield/","section":"publication","summary":"We introduce a modern Hopfield network with continuous states and a corresponding update rule. The new update rule is equivalent to the attention mechanism used in transformers. Published at ICLR 2021.","tags":["Modern Hopfield Networks","Self-Attention","Transformer","Associative Memory","Deep Learning"],"title":"Hopfield Networks is All You Need","type":"publication"},{"authors":["Michael Widrich","Bernhard Schäfl","Milena Pavlović","Hubert Ramsauer","Lukas Gruber","Markus Holzleitner","Johannes Brandstetter","Geir Kjetil Sandve","Victor Greiff","Sepp Hochreiter","Günter Klambauer"],"categories":["Modern Hopfield Networks"],"content":"","date":1594637717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594637717,"objectID":"287fc1fa07e1c76a1ec5985ba9bc4513","permalink":"https://brandstetter-johannes.github.io/publication/widrich-2020-hopfield/","publishdate":"2020-07-13T12:55:17+02:00","relpermalink":"/publication/widrich-2020-hopfield/","section":"publication","summary":"We exploit the storage capacity of modern Hopfield networks to solve a challenging multiple instance learning (MIL) problem in computational biology: immune repertoire classification. Published at NeurIPS 2020 (Spotlight).","tags":["Modern Hopfield Networks","Self-Attention","Transformer","Associative Memory","Immune Repertoire Classification","Deep Learning"],"title":"Modern hopfield networks and attention for immune repertoire classification","type":"publication"},{"authors":["Johannes Brandstetter","CMS Collaboration"],"categories":["High Energy Physics"],"content":"","date":1538045717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538045717,"objectID":"74df16331f7d4e5efcd011cb81049025","permalink":"https://brandstetter-johannes.github.io/publication/cms-combined/","publishdate":"2018-09-27T12:55:17+02:00","relpermalink":"/publication/cms-combined/","section":"publication","summary":"In this work, we present combined measurements of the production and decay rates of the Higgs boson, as well as its couplings to vector bosons and fermions. Published at Eur. Phys. J. C 79 (2019) 421.","tags":["High Energy Physics","Higgs Boson Physics","CMS Collaboration","CERN","Statistical Modeling"],"title":"Combined measurements of Higgs boson couplings in proton-proton collisions at $\\sqrt{s}=$ 13 TeV","type":"publication"},{"authors":null,"categories":null,"content":"After switching from High Energy Physics to Deep Learning, I started working in Reinforcement Learning before pivoting towards Associative Memories and modern Transformer networks. Recent years have shown that scalable ideas, improving the datasets, and clever engineering are the ingredients for ever better Deep Learning models. This totally coincides with my experience, and \u0026ndash; needless to say \u0026ndash; I will continue working on general large-scale Deep Learning directions.\nIn Reinforcement Learning, we introduce RUDDER a novel model-free RL approach to overcome delayed reward problems. RUDDER directly and efficiently assigns credit to reward-causing state-action pairs and thereby speeds up learning in model-free reinforcement learning with delayed rewards dramatically. I have written a lengthy blogpost on the RUDDER idea. With Align-RUDDER, we extended the RUDDER framework by assuming that episodes with high rewards are given as demonstrations. Finally, we prove converge for actor-critic methods like RUDDER or PPO.\nIn Hopfield Networks is All You Need, we introduced a new energy function and a corresponding new update rule which is guaranteed to converge to a local minimum of the energy function. The new modern Hopfield Network with continuous states keeps the characteristics of its discrete counterparts, i.e., exponential storage capacity and fast convergence. Due to its continuous states this new modern Hopfield Network is differentiable and can be integrated into deep learning architectures. Typically patterns are retrieved after one update which is compatible with activating the layers of deep networks. Surprisingly, the new update rule is the attention mechanism of transformer networks introduced in Attention Is All You Need. I have written a lenthy blogpost on modern Hopfield Networks. One SOTA application of modern Hopfield Networks can be found in our paper Modern Hopfield Networks and Attention for Immune Repertoire Classification. Here, the high storage capacity of modern Hopfield Networks is exploited to solve a challenging multiple instance learning (MIL) problem in computational biology called immune repertoire classification. Finally, we found out that linearized attention models, such as the Performer idea resemble the update rule of classical Hopfield Networks. Our blogpost was published in the ICLR2022 blogpost track.\nI will keep working on understanding large scale architectures. There are quite a few interesting projects I am involved in.\nStay stuned for new work ","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"ceaa985865bea43cb19782f318319faf","permalink":"https://brandstetter-johannes.github.io/research/deep-learning/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/research/deep-learning/","section":"research","summary":"After switching from High Energy Physics to Deep Learning, I started working in Reinforcement Learning before pivoting towards Associative Memories and modern Transformer networks. Recent years have shown that scalable ideas, improving the datasets, and clever engineering are the ingredients for ever better Deep Learning models. This totally coincides with my experience, and -- needless to say -- I will continue working on general large-scale Deep Learning directions.","tags":["Deep Learning","Modern Hopfield Networks","Self-Attention","Transformer","Associative Memory","Reinforcement Learning","Delayed Rewards"],"title":"General Deep Learning","type":"research"},{"authors":["José A. Arjona-Medina","Michael Gillhofer","Michael Widrich","Thomas Unterthiner","Johannes Brandstetter","Sepp Hochreiter"],"categories":["Reinforcement Learning"],"content":"","date":1529492117,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529492117,"objectID":"20e22faeac2f4cdf5ffcc2d30c8e6624","permalink":"https://brandstetter-johannes.github.io/publication/medina-2018-rudder/","publishdate":"2018-06-20T12:55:17+02:00","relpermalink":"/publication/medina-2018-rudder/","section":"publication","summary":"We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes. Published at NeurIPS 2019.","tags":["Reinforcement Learning","Delayed Rewards","Deep Learning"],"title":"RUDDER -- Return Decomposition for Delayed Rewards","type":"publication"},{"authors":["Johannes Brandstetter","CMS Collaboration"],"categories":["High Energy Physics"],"content":"","date":1528541717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528541717,"objectID":"8cfbba052a7b7cfa6200bf3cc0b59ef1","permalink":"https://brandstetter-johannes.github.io/publication/cms-leptoquark/","publishdate":"2018-06-09T12:55:17+02:00","relpermalink":"/publication/cms-leptoquark/","section":"publication","summary":"This paper reports a search for a singly produced third-generation scalar leptoquark decaying to a $\\tau$ lepton and a bottom quark. Published at Journal of High Energy Physics (2018).","tags":["High Energy Physics","Leptoquark","CMS Collaboration","Fake Factor Method","CERN","Statistical Modeling"],"title":"Search for a singly produced third-generation scalar leptoquark decaying to a $\\tau$ lepton and a bottom quark in proton-proton collisions at $\\sqrt{s}=$ 13 TeV","type":"publication"},{"authors":["Johannes Brandstetter"],"categories":["High Energy Physics"],"content":"","date":1527245717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527245717,"objectID":"9bde64dee4dcad3061655e688da1027a","permalink":"https://brandstetter-johannes.github.io/publication/brandstetter-2018-diss/","publishdate":"2018-05-25T12:55:17+02:00","relpermalink":"/publication/brandstetter-2018-diss/","section":"publication","summary":"My PhD thesis on neutral Higgs bosons and Z bosons. Published at CERN-THESIS-2018-066.","tags":["High Energy Physics","Higgs Boson Physics","CMS Collaboration","Supersymmetry","Fake Factor Method","CERN","Statistical Modeling"],"title":"Neutral Higgs boson and Z boson decays into pairs of tau leptons with the CMS detector","type":"publication"},{"authors":["Johannes Brandstetter","CMS Collaboration"],"categories":["High Energy Physics"],"content":"","date":1521284117,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521284117,"objectID":"4ebfbe59bfe6deba080a9eaabad0b773","permalink":"https://brandstetter-johannes.github.io/publication/cms-higgs-mssm/","publishdate":"2018-03-17T12:55:17+02:00","relpermalink":"/publication/cms-higgs-mssm/","section":"publication","summary":"This paper reports a search for additional neutral Higgs bosons in the $\\tau\\tau$ final state in proton-proton collisions at the LHC. The search is performed in the context of the minimal supersymmetric extension of the standard model (MSSM). Published at Journal of High Energy Physics (2018).","tags":["High Energy Physics","Higgs Boson Physics","CMS Collaboration","Supersymmetry","Fake Factor Method","CERN","Statistical Modeling"],"title":"Search for additional neutral MSSM Higgs bosons in the $\\tau\\tau$ final state in proton-proton collisions at $\\sqrt{s}=$ 13 TeV","type":"publication"},{"authors":["Johannes Brandstetter","CMS Collaboration"],"categories":["High Energy Physics"],"content":"","date":1515581717,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515581717,"objectID":"94b157ddd0c0792f3a02a597e39c7921","permalink":"https://brandstetter-johannes.github.io/publication/cms-ztautau/","publishdate":"2018-01-10T12:55:17+02:00","relpermalink":"/publication/cms-ztautau/","section":"publication","summary":"In this paper, we report a measurement of the the $Z/ \\gamma^* \\rightarrow \\tau \\tau$ cross section and validation of the fake factor background estimation method. Published at European Physical Journal. C, Particles and Fields (2018).","tags":["High Energy Physics","Z Boson Physics","CMS Collaboration","Fake Factor Method","CERN","Statistical Modeling"],"title":"Measurement of the $Z/ \\gamma^* \\rightarrow \\tau \\tau$ cross section in pp collisions at $\\sqrt{s}=$ 13 TeV and validation of $\\tau$ lepton analysis techniques","type":"publication"},{"authors":["Johannes Brandstetter","CMS Collaboration"],"categories":["High Energy Physics"],"content":"","date":1501584917,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501584917,"objectID":"5d678f11b9b4cf8666b4b5e1cf625c0f","permalink":"https://brandstetter-johannes.github.io/publication/cms-higgs/","publishdate":"2017-08-01T12:55:17+02:00","relpermalink":"/publication/cms-higgs/","section":"publication","summary":"We report the first direct observation of the Higgs boson decaying into a pair of fermions. Published at Eur. \tPhys. Lett. B 779 (2018) 283.","tags":["High Energy Physics","Higgs Boson Physics","CMS Collaboration","CERN","Statistical Modeling"],"title":"Observation of the Higgs boson decay to a pair of tau leptons with the CMS detector","type":"publication"},{"authors":null,"categories":null,"content":"I have spent five years (including my PhD) working in the CMS Collaboration at CERN \u0026ndash; the European Organization for Nuclear Research. Most of my research was dedicated to Higgs boson physics.\nMy work was on statistical and physics analysis which means making sense of terabytes of data collected by the CMS detector at the Large Hadron Collider (LHC). Doing physics analysis also means to make the unvisible visible. For example, neither the Higgs boson itself nor the many \u0026lsquo;\u0026lsquo;background\u0026rsquo;\u0026rsquo; processes can be measured directly. Worse, even most of the decay products cannot be measured directly. Consequently, everything needs to be reconstructed from terabytes of detector data by using sophisticated software tools and physical simulations.\nOne of my biggest contributions was to develope the so-called Fake Factor Method. The fake factor method is a fully data-driven background estimation method for the most important background processes of the standard model Higgs boson and potential supersymmetric twins. Fully data-driven means that only directly recorded collision data is needed in contrast to standard background estimation methods which require tons of simulation data. This makes the fake factor method more robust and scalable. The fake factor method is still used as go-to tool in modern Higgs boson analyses.\n","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"580974cca7861d0b790e5a631a75ab7e","permalink":"https://brandstetter-johannes.github.io/research/high-energy-physics/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/research/high-energy-physics/","section":"research","summary":"I have spent five years (including my PhD) working in the CMS Collaboration at CERN. Most of my research was dedicated to Higgs boson physics.","tags":["CERN","CMS Collaboration","High Energy Physics","Higgs Boson Physics","Supersymmetry","Fake Factor Method"],"title":"High Energy Physics","type":"research"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://brandstetter-johannes.github.io/contact/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c9b5771543b03b8149b612b630936a56","permalink":"https://brandstetter-johannes.github.io/experience/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/experience/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"9eb50f9088083bebcb7e4cf99e22b9ed","permalink":"https://brandstetter-johannes.github.io/news/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]