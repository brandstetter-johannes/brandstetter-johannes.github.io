<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>PyTorch Lightning | Johannes Brandstetter</title><link>https://brandstetter-johannes.github.io/tag/pytorch-lightning/</link><atom:link href="https://brandstetter-johannes.github.io/tag/pytorch-lightning/index.xml" rel="self" type="application/rss+xml"/><description>PyTorch Lightning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 13 Jun 2022 00:00:00 +0000</lastBuildDate><image><url>https://brandstetter-johannes.github.io/images/icon_hu3751bf80caad288f7f32134a8ecae786_4078_512x512_fill_lanczos_center_3.png</url><title>PyTorch Lightning</title><link>https://brandstetter-johannes.github.io/tag/pytorch-lightning/</link></image><item><title>Deep Learning Tutorials Translated to JAX with Flax</title><link>https://brandstetter-johannes.github.io/post/uvadlc-tutorials-jax/</link><pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/post/uvadlc-tutorials-jax/</guid><description>&lt;figure>
&lt;img src="FrontImgv2.png" width="100%">
&lt;figcaption>&lt;b>Figure 1&lt;/b>: We have recently translated our Deep Learning Tutorials to JAX with Flax, offering 1-to-1 translations between PyTorch (Lightning) and JAX with Flax.&lt;/figcaption>
&lt;/figure>
&lt;p>PyTorch is one of the most popular Deep Learning frameworks using in research on machine learning. However, another framework, JAX, has recently gained more and more popularity. But why should you learn JAX, if there are already so many other deep learning frameworks like PyTorch and TensorFlow? The short answer: because it can be extremely fast. For instance, a small GoogleNet on CIFAR10, which we discuss in detail in Tutorial 5, can be trained in JAX 3x faster than in PyTorch with a similar setup. Note that for larger models, larger batch sizes, or smaller GPUs, a considerably smaller speedup is expected, and the code has not been designed for benchmarking. Nonetheless, JAX enables this speedup by compiling functions and numerical programs for accelerators (GPU/TPU) just in time, finding the optimal utilization of the hardware. Frameworks with dynamic computation graphs like PyTorch cannot achieve the same efficiency since they cannot anticipate the next operations before the user calls them. For example, in an Inception block of GoogleNet, we apply multiple convolutional layers in parallel on the same input. JAX can optimize the execution of this layer by compiling the whole forward pass for the available accelerator and parallelizing the convolutions where possible. In contrast, when calling the first convolutional layer in PyTorch, the framework does not know that multiple convolutions on the same feature map will follow. It sends each operation one by one to the GPU, and can only adapt the execution after seeing the next Python calls. Hence, JAX can make more efficient use of the GPU than, for instance, PyTorch.&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Are you interested in learning JAX with Flax? We have translated our popular Deep Learning tutorials on CNNs, GNNs, (Vision) Transformers, and more from PyTorch to JAX+Flax, with considerable speedups for smaller models! Check them out here: &lt;a href="https://t.co/x38ZWrjZrT">https://t.co/x38ZWrjZrT&lt;/a>&lt;br>ðŸ§µ 1/12 &lt;a href="https://t.co/pHvQQC4WJM">pic.twitter.com/pHvQQC4WJM&lt;/a>&lt;/p>&amp;mdash; Phillip Lippe (@phillip_lippe) &lt;a href="https://twitter.com/phillip_lippe/status/1536340878960173057?ref_src=twsrc%5Etfw">June 13, 2022&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>Because of that, we have recently translated our &lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial2/Introduction_to_JAX.html" target="_blank" rel="noopener">popular Deep Learning Tutorials&lt;/a> of the DL course at the University of Amsterdam from PyTorch with PyTorch Lightning to JAX with Flax. These 1-to-1 translations allow you to see implementations of common models side-by-side, experience how you can go from PyTorch to JAX, and are more guided through a model creation. Furthermore, we also provide a simple introduction to JAX with Flax, building a very small neural network with basic JAX tools. Check them out on our &lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/" target="_blank" rel="noopener">RTD website&lt;/a>!&lt;/p></description></item><item><title>Official Collaboration with PyTorch Lightning</title><link>https://brandstetter-johannes.github.io/post/uvadlc-tutorials-lightning/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/post/uvadlc-tutorials-lightning/</guid><description>&lt;figure>
&lt;img src="pytorch_lightning_gif.gif" width="80%">
&lt;figcaption>&lt;b>Figure 1&lt;/b>: We have collaborated with the PyTorch Lightning team to integrate our Deep Learning Tutorials as official tutorials in their documentation! Figure credit: PyTorch Lightning Team&lt;/figcaption>
&lt;/figure>
&lt;p>PyTorch Lightning is a wrapper around PyTorch, allowing you to reduce your code overhead for training and get advanced features like multi-GPU training for free. We have used PyTorch Lightning several times in our &lt;a href="https://uvadlc-notebooks.readthedocs.io/en/latest/" target="_blank" rel="noopener">Deep Learning Tutorials&lt;/a> at the University of Amsterdam, and have seen a great response from our students!&lt;/p>
&lt;p>Today, we are happy to &lt;a href="https://devblog.pytorchlightning.ai/lightning-tutorials-in-collaboration-with-the-university-of-amsterdam-uva-2499eaa0caad" target="_blank" rel="noopener">announce&lt;/a> that we have collaborated with the PyTorch Lightning team to integrate our tutorials as official Lightning tutorials in their documentation. Check them out &lt;a href="https://pytorch-lightning.readthedocs.io/en/latest/notebooks/course_UvA-DL/01-introduction-to-pytorch.html" target="_blank" rel="noopener">here&lt;/a> to get started with your Lightning experience!&lt;/p>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">ðŸŽ‰ In the newest v1.5 release, we collaborated with &lt;a href="https://twitter.com/UvA_Amsterdam?ref_src=twsrc%5Etfw">@UvA_Amsterdam&lt;/a> to integrate their PyTorch Lightning Deep Learning Course as an interactive end-to-end course within our Lighting documentation.&lt;br> &lt;br>ðŸŽ“ Master &lt;a href="https://twitter.com/hashtag/deeplearning?src=hash&amp;amp;ref_src=twsrc%5Etfw">#deeplearning&lt;/a> with this new tutorial series! &lt;a href="https://t.co/LxjqdXJF8o">https://t.co/LxjqdXJF8o&lt;/a> &lt;a href="https://t.co/eQu9jwUtQW">pic.twitter.com/eQu9jwUtQW&lt;/a>&lt;/p>&amp;mdash; PyTorch Lightning (@PyTorchLightnin) &lt;a href="https://twitter.com/PyTorchLightnin/status/1456355590150688768?ref_src=twsrc%5Etfw">November 4, 2021&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script></description></item></channel></rss>