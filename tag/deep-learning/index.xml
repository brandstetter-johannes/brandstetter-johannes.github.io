<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning | Johannes Brandstetter</title><link>https://brandstetter-johannes.github.io/tag/deep-learning/</link><atom:link href="https://brandstetter-johannes.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>Deep Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 06 Jun 2024 12:55:17 +0200</lastBuildDate><image><url>https://brandstetter-johannes.github.io/images/icon_hu3751bf80caad288f7f32134a8ecae786_4078_512x512_fill_lanczos_center_3.png</url><title>Deep Learning</title><link>https://brandstetter-johannes.github.io/tag/deep-learning/</link></image><item><title>Vision-LSTM -- xLSTM as Generic Vision Backbone</title><link>https://brandstetter-johannes.github.io/publication/alkin-2024-vision-lstm/</link><pubDate>Thu, 06 Jun 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/alkin-2024-vision-lstm/</guid><description/></item><item><title>Aurora -- A foundation Model of the Atmosphere</title><link>https://brandstetter-johannes.github.io/publication/bodnar-2024-aurora/</link><pubDate>Mon, 20 May 2024 09:47:56 +0100</pubDate><guid>https://brandstetter-johannes.github.io/publication/bodnar-2024-aurora/</guid><description/></item><item><title>xLSTM -- Extended Long Short-Term Memory</title><link>https://brandstetter-johannes.github.io/publication/beck-2024-xlstm/</link><pubDate>Tue, 07 May 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/beck-2024-xlstm/</guid><description/></item><item><title>Geometry-Informed Neural Networks</title><link>https://brandstetter-johannes.github.io/publication/berzins-2024-ginns/</link><pubDate>Wed, 21 Feb 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/berzins-2024-ginns/</guid><description/></item><item><title>Universal Physics Transformers -- A Framework For Efficiently Scaling Neural Operators</title><link>https://brandstetter-johannes.github.io/publication/alkin-2024-upt/</link><pubDate>Mon, 19 Feb 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/alkin-2024-upt/</guid><description/></item><item><title>Mim-refiner -- A contrastive learning boost from intermediate pre-trained representations</title><link>https://brandstetter-johannes.github.io/publication/alkin-2024-mimrefiner/</link><pubDate>Thu, 15 Feb 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/alkin-2024-mimrefiner/</guid><description/></item><item><title>We identify particle clustering originating from tensile instabilities as one of the primary pitfalls. Based on these insights, we enhance both training and rollout inference of GNN-based simulators with varying components from standard SPH solvers, including pressure, viscous, and external force components.</title><link>https://brandstetter-johannes.github.io/publication/toshev-2024-neuralsph/</link><pubDate>Fri, 09 Feb 2024 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/toshev-2024-neuralsph/</guid><description/></item><item><title>Lie Point Symmetries and Physics-Informed Networks</title><link>https://brandstetter-johannes.github.io/publication/akhound-sadegh-2023-lps-pinn/</link><pubDate>Tue, 07 Nov 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/akhound-sadegh-2023-lps-pinn/</guid><description/></item><item><title>Data-Driven Simulations</title><link>https://brandstetter-johannes.github.io/research/data-driven-simulations/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/research/data-driven-simulations/</guid><description>&lt;p>My years in Amsterdam &amp;ndash; first at the &lt;a href="https://amlab.science.uva.nl/" target="_blank" rel="noopener">Amsterdam Machine Learning Lab&lt;/a> and then 2 years at in the &lt;a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-ai4science/" target="_blank" rel="noopener">AI4Science lab&lt;/a> at Microsoft Research &amp;ndash; have shaped my research vision. I am firmly convinced that AI is on the cusp of disrupting simulations at industry-scale. Therefore, I have started a new group at JKU Linz which has strong computer vision, numerical simulation, and engineering components. We want to advance data-driven simulations at industry-scale, and place the Austrian industry engine Linz as a center for doing that.&lt;/p>
&lt;h1 id="about-my-research">About my research&lt;/h1>
&lt;p>Working in &lt;a href="https://brandstetter-johannes.github.io/tag/high-energy-physics/" target="_blank" rel="noopener">high energy physics&lt;/a> means to make the invisible visible. When we published for example the &lt;a href="https://brandstetter-johannes.github.io/publication/cms-higgs/" target="_blank" rel="noopener">first direct observation of the Higgs boson decaying into pairs of tau leptons&lt;/a> non of the processes, i.e., the Higgs boson signal itself nor the many &amp;lsquo;&amp;lsquo;background&amp;rsquo;&amp;rsquo; processes, was measured directly. Everything needed to be reconstructed from terabytes of detector data, and &amp;ndash; even more fascinating to me &amp;ndash; by using sophisticated physical simulations.&lt;/p>
&lt;p>My first contact with simulations in the context of Deep Learning was through the work on
&lt;a href="https://brandstetter-johannes.github.io/publication/mayr-2021-bgnn" target="_blank" rel="noopener">Boundary graph neural networks for 3D simulations&lt;/a>, which was financed by the &lt;a href="https://www.ffg.at/en" target="_blank" rel="noopener">Austrian Research Promotion Agency (FFG)&lt;/a> after winning a research grant together with &lt;a href="https://www.aspherix-dem.com/" target="_blank" rel="noopener">DCS computing&lt;/a>.&lt;/p>
&lt;p>During my years in Amsterdam I fully pivoted towards neural modeling of &lt;a href="https://brandstetter-johannes.github.io/tag/neural-solvers/" target="_blank" rel="noopener">partial differential equations (PDEs)&lt;/a>. For example:&lt;/p>
&lt;ul>
&lt;li>In &lt;a href="https://brandstetter-johannes.github.io/publication/brandstetter-2022-mpnn/" target="_blank" rel="noopener">Message Passing Neural PDE Solvers&lt;/a>, we replaced all heuristically designed components in numerical PDE solvers with backprop-optimized neural function approximators.&lt;/li>
&lt;li>We studied how to use &lt;a href="https://brandstetter-johannes.github.io/publication/brandstetter-2022-lpsda/" target="_blank" rel="noopener">Lie Point Symmetries of PDEs&lt;/a> to improve sample complexity of neural PDE solvers.&lt;/li>
&lt;li>In &lt;a href="https://brandstetter-johannes.github.io/publication/brandstetter-2022-clifford/" target="_blank" rel="noopener">Clifford Neural Layers for PDE Modeling&lt;/a> and &lt;a href="https://brandstetter-johannes.github.io/publication/ruhe-2023-cgans/" target="_blank" rel="noopener">Geometric Clifford Algebra Networks&lt;/a>, we introduced neural network layers for PDE modeling, which are based on operations on composite objects of scalars, vectors, and higher order objects.&lt;/li>
&lt;li>In &lt;a href="https://brandstetter-johannes.github.io/publication/gupta-2022-pdearena/" target="_blank" rel="noopener">Towards Multi-spatiotemporal-scale Generalized PDE Modeling&lt;/a>, we introduce &lt;a href="https://microsoft.github.io/pdearena/" target="_blank" rel="noopener">PDEArena&lt;/a>, a modern PyTorch Lightning-based deep learning framework for neural PDE modeling with a plethora of state-of-the art neural PDE surrogates and various temporal PDEs in 1-, 2-, and 3-spatial dimensions.&lt;/li>
&lt;li>In &lt;a href="https://brandstetter-johannes.github.io/publication/nguyen-2023-climax/" target="_blank" rel="noopener">ClimaX&lt;/a>, we developed a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets. ClimaX is the first foundation model for weather and climate.&lt;/li>
&lt;/ul>
&lt;h1 id="how-to-disrupt-simulations-at-industry-scale">How to disrupt simulations at industry-scale&lt;/h1>
&lt;p>Recent years have shown that scalable ideas, improving the datasets, and clever engineering are the ingredients for ever better Deep Learning systems. LLMs and large-scale computer vision models have given birth to the heavily overused term &amp;ldquo;foundation model&amp;rdquo;.
After language and computer vision, foundation models have spread over to various disciplines, with &lt;a href="https://www.nature.com/articles/s41586-021-03819-2" target="_blank" rel="noopener">AlphaFold&lt;/a> as very prominent example.&lt;/p>
&lt;p>I want to go one step further, and bring foundation models to the every-day engineering world, and in doing so, disrupt how our industry works. Every day thousands and thousands of compute hours are spent on turbulence modeling, simulations of fluid or air flows, heat transfer in materials, traffic flows, and many more. Many of these processes follow similar underlying patterns, but yet need different and extremely specialized softward to simulate. Even worse, for different parameter settings the costly simulations need to be run at full length from scratch.&lt;/p>
&lt;p>This is what I want to change! Deep Learning techniques are ready to develope models which run simulations in seconds instead of days or even weeks. The hardware is ready to digest high-resolutional industry-scale inputs, e.g., 3D meshes or images, and subsequently sets the stage to train Deep Learning models at scale.&lt;/p>
&lt;p>But what about the data you might ask? Don&amp;rsquo;t we need high-quality ground truth data, which usually must come from the very simulations we are going to replace, presenting us with a proverbial chicken-and-egg problem? Luckily, many of the above mentioned processes have common underlying dynamics &amp;ndash; similar to how different languages share structure and grammar. Simulation data is abundant, we just need to use the right ones, and many of that.&lt;/p>
&lt;p>Personally, I have experienced modern computer vision as one of the driving tools for developing data-driven simulations. But it will take much more. It will need expertise in numerical simulations, geometric and physics-informed deep learning, and engineering at scale. We have already put together a very strong interdisciplinary team with different expertise, and keep hiring. The journey is going to be exciting.&lt;/p>
&lt;h2 id="stay-stuned-first-works-will-appear-soon">Stay stuned, first works will appear soon&lt;/h2></description></item><item><title>PDE-Refiner - Achieving Accurate Long Rollouts with Neural PDE Solvers</title><link>https://brandstetter-johannes.github.io/publication/lippe-2023-pderefiner/</link><pubDate>Thu, 10 Aug 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/lippe-2023-pderefiner/</guid><description/></item><item><title>Learning Lagrangian Fluid Mechanics with E(3)-Equivariant Graph Neural Networks</title><link>https://brandstetter-johannes.github.io/publication/toshev-2023-lagrangian/</link><pubDate>Wed, 24 May 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/toshev-2023-lagrangian/</guid><description/></item><item><title>Learning Lagrangian Fluid Mechanics with E(3)-Equivariant Graph Neural Networks</title><link>https://brandstetter-johannes.github.io/publication/toshev-2024-jaxsph/</link><pubDate>Wed, 24 May 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/toshev-2024-jaxsph/</guid><description/></item><item><title>Clifford Group Equivariant Neural Networks</title><link>https://brandstetter-johannes.github.io/publication/ruhe-2023-cgenns/</link><pubDate>Thu, 18 May 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/ruhe-2023-cgenns/</guid><description/></item><item><title>Geometric Clifford Algebra Networks</title><link>https://brandstetter-johannes.github.io/publication/ruhe-2023-cgans/</link><pubDate>Mon, 13 Feb 2023 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/ruhe-2023-cgans/</guid><description/></item><item><title>ClimaX -- A foundation model for weather and climate</title><link>https://brandstetter-johannes.github.io/publication/nguyen-2023-climax/</link><pubDate>Tue, 24 Jan 2023 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/publication/nguyen-2023-climax/</guid><description/></item><item><title>Towards Multi-spatiotemporal-scale Generalized PDE Modeling</title><link>https://brandstetter-johannes.github.io/publication/gupta-2022-pdearena/</link><pubDate>Fri, 30 Sep 2022 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/publication/gupta-2022-pdearena/</guid><description/></item><item><title>Clifford Neural Layers for PDE Modeling</title><link>https://brandstetter-johannes.github.io/publication/brandstetter-2022-clifford/</link><pubDate>Thu, 08 Sep 2022 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/brandstetter-2022-clifford/</guid><description/></item><item><title>Lie Point Symmetry Data Augmentation for Neural PDE Solvers</title><link>https://brandstetter-johannes.github.io/publication/brandstetter-2022-lpsda/</link><pubDate>Tue, 15 Feb 2022 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/brandstetter-2022-lpsda/</guid><description/></item><item><title>Message Passing Neural PDE Solvers</title><link>https://brandstetter-johannes.github.io/publication/brandstetter-2022-mpnn/</link><pubDate>Mon, 07 Feb 2022 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/brandstetter-2022-mpnn/</guid><description/></item><item><title>Geometric and Physical Quantities Improve E(3) Equivariant Message Passing</title><link>https://brandstetter-johannes.github.io/publication/brandstetter-2021-segnn/</link><pubDate>Wed, 06 Oct 2021 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/brandstetter-2021-segnn/</guid><description/></item><item><title>Boundary Graph Neural Networks for 3D Simulations</title><link>https://brandstetter-johannes.github.io/publication/mayr-2021-bgnn/</link><pubDate>Mon, 21 Jun 2021 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/mayr-2021-bgnn/</guid><description/></item><item><title>Geometric Deep Learning</title><link>https://brandstetter-johannes.github.io/research/geometric-deep-learning/</link><pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/research/geometric-deep-learning/</guid><description>&lt;p>My passion for Geometric Deep Learning can be unmistakenly traced back to my physics background. Doing a PhD in high energy physics was confronting me with the standard model of particle physics on a daily basis. Under the hood, the standard model is a gauge quantum field theory which contains the internal symmetries of the unitary product group $SU(3) \times SU(2) \times U(1)$. The theory is commonly viewed as describing the fundamental set of particles &amp;ndash; leptons, quarks, gauge bosons and the Higgs boson &amp;ndash; as well as their interactions.&lt;/p>
&lt;p>Unsurprisingly, I have been deeply attracted to the field of &lt;a href="https://geometricdeeplearning.com/" target="_blank" rel="noopener">Geometric Deep Learning&lt;/a> which has groups, gauges, and graphs at its core.
I have contributed to the fields of &lt;a href="https://brandstetter-johannes.github.io/tag/graph-neural-networks/" target="_blank" rel="noopener">graph neural networks&lt;/a>, &lt;a href="https://brandstetter-johannes.github.io/tag/equivariance/" target="_blank" rel="noopener">equivariant architectures&lt;/a>, and &lt;a href="https://brandstetter-johannes.github.io/tag/neural-solvers/" target="_blank" rel="noopener">neural PDE solvers&lt;/a>. Furthermore, I have lead efforts to introduce &lt;a href="https://brandstetter-johannes.github.io/publication/brandstetter-2022-lpsda/" target="_blank" rel="noopener">Lie Point Symmetries&lt;/a>, and, most recently, &lt;a href="https://brandstetter-johannes.github.io/tag/clifford-algebras/" target="_blank" rel="noopener">Clifford (Geometric) Algebras&lt;/a> into the Deep Learning community.&lt;/p>
&lt;p>For my group &amp;ldquo;AI for data-driven simulations&amp;rdquo;, Geometric Deep Learning techniques will play an important role when working on grids, graphs, meshes, or with complex geometries in general. Recent trends on implicit parametrization, generative modeling, and latent space dynamics are very appealing as well.&lt;/p>
&lt;h2 id="stay-stuned-for-future-work">Stay stuned for future work&lt;/h2></description></item><item><title>Looking at the Performer from a Hopfield point of view</title><link>https://brandstetter-johannes.github.io/publication/brandstetter-2020-performer/</link><pubDate>Sat, 12 Dec 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/brandstetter-2020-performer/</guid><description/></item><item><title>Convergence proof for actor-critic methods applied to ppo and rudder</title><link>https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/</link><pubDate>Wed, 02 Dec 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/</guid><description/></item><item><title>Align-RUDDER -- Learning From Few Demonstrations by Reward Redistribution</title><link>https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/</link><pubDate>Tue, 29 Sep 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/</guid><description/></item><item><title>Hopfield Networks is All You Need</title><link>https://brandstetter-johannes.github.io/publication/ramsauer-2020-hopfield/</link><pubDate>Thu, 16 Jul 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/ramsauer-2020-hopfield/</guid><description/></item><item><title>Modern hopfield networks and attention for immune repertoire classification</title><link>https://brandstetter-johannes.github.io/publication/widrich-2020-hopfield/</link><pubDate>Mon, 13 Jul 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/widrich-2020-hopfield/</guid><description/></item><item><title>General Deep Learning</title><link>https://brandstetter-johannes.github.io/research/deep-learning/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://brandstetter-johannes.github.io/research/deep-learning/</guid><description>&lt;p>After switching from High Energy Physics to Deep Learning, I started working in Reinforcement Learning before pivoting towards Associative Memories and modern Transformer networks. Recent years have shown that scalable ideas, improving the datasets, and clever engineering are the ingredients for ever better Deep Learning models. This totally coincides with my experience, and &amp;ndash; needless to say &amp;ndash; I will continue working on general large-scale Deep Learning directions.&lt;/p>
&lt;p>In &lt;a href="https://brandstetter-johannes.github.io/tag/reinforcement-learning/" target="_blank" rel="noopener">Reinforcement Learning&lt;/a>, we introduce &lt;a href="https://brandstetter-johannes.github.io/publication/medina-2018-rudder/" target="_blank" rel="noopener">RUDDER&lt;/a> a novel model-free RL approach to overcome delayed reward problems. RUDDER directly and efficiently assigns credit to reward-causing state-action pairs and thereby speeds up learning in model-free reinforcement learning with delayed rewards dramatically. I have written a lengthy &lt;a href="https://ml-jku.github.io/rudder/" target="_blank" rel="noopener">blogpost&lt;/a> on the RUDDER idea. With &lt;a href="https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/" target="_blank" rel="noopener">Align-RUDDER&lt;/a>, we extended the RUDDER framework by assuming that episodes with high rewards are given as demonstrations. Finally, we &lt;a href="https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/" target="_blank" rel="noopener">prove converge for actor-critic methods like RUDDER or PPO&lt;/a>.&lt;/p>
&lt;p>In &lt;a href="https://brandstetter-johannes.github.io/publication/ramsauer-2020-hopfield/" target="_blank" rel="noopener">Hopfield Networks is All You Need&lt;/a>, we introduced a new energy function and a corresponding new update rule which is guaranteed to converge to a local minimum of the energy function. The new modern Hopfield Network with continuous states keeps the characteristics of its discrete counterparts, i.e., exponential storage capacity and fast convergence. Due to its continuous states this new modern Hopfield Network is differentiable and can be integrated into deep learning architectures. Typically patterns are retrieved after one update which is compatible with activating the layers of deep networks. Surprisingly, the new update rule is the attention mechanism of transformer networks introduced in &lt;a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Attention Is All You Need&lt;/a>. I have written a lenthy &lt;a href="https://ml-jku.github.io/hopfield-layers/" target="_blank" rel="noopener">blogpost&lt;/a> on modern Hopfield Networks. One SOTA application of modern Hopfield Networks can be found in our paper &lt;a href="https://brandstetter-johannes.github.io/publication/widrich-2020-hopfield/" target="_blank" rel="noopener">Modern Hopfield Networks and Attention for Immune Repertoire Classification&lt;/a>. Here, the high storage capacity of modern Hopfield Networks is exploited to solve a challenging multiple instance learning (MIL) problem in computational biology called immune repertoire classification. Finally, we found out that linearized attention models, such as the &lt;a href="https://arxiv.org/abs/2009.14794" target="_blank" rel="noopener">Performer idea&lt;/a> resemble the update rule of classical Hopfield Networks. &lt;a href="https://ml-jku.github.io/blog-post-performer/" target="_blank" rel="noopener">Our blogpost&lt;/a> was published in the ICLR2022 blogpost track.&lt;/p>
&lt;p>I will keep working on understanding large scale architectures. There are quite a few interesting projects I am involved in.&lt;/p>
&lt;h2 id="stay-stuned-for-new-work">Stay stuned for new work&lt;/h2></description></item><item><title>RUDDER -- Return Decomposition for Delayed Rewards</title><link>https://brandstetter-johannes.github.io/publication/medina-2018-rudder/</link><pubDate>Wed, 20 Jun 2018 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/medina-2018-rudder/</guid><description/></item></channel></rss>