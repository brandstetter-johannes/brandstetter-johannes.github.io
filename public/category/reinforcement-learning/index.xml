<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning | Johannes Brandstetter</title><link>https://brandstetter-johannes.github.io/category/reinforcement-learning/</link><atom:link href="https://brandstetter-johannes.github.io/category/reinforcement-learning/index.xml" rel="self" type="application/rss+xml"/><description>Reinforcement Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 02 Dec 2020 12:55:17 +0200</lastBuildDate><image><url>https://brandstetter-johannes.github.io/images/icon_hu3751bf80caad288f7f32134a8ecae786_4078_512x512_fill_lanczos_center_3.png</url><title>Reinforcement Learning</title><link>https://brandstetter-johannes.github.io/category/reinforcement-learning/</link></image><item><title>Convergence proof for actor-critic methods applied to ppo and rudder</title><link>https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/</link><pubDate>Wed, 02 Dec 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/holzleitner-2020-convergence/</guid><description/></item><item><title>Align-RUDDER -- Learning From Few Demonstrations by Reward Redistribution</title><link>https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/</link><pubDate>Tue, 29 Sep 2020 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/patil-2020-alignrudder/</guid><description/></item><item><title>RUDDER -- Return Decomposition for Delayed Rewards</title><link>https://brandstetter-johannes.github.io/publication/medina-2018-rudder/</link><pubDate>Wed, 20 Jun 2018 12:55:17 +0200</pubDate><guid>https://brandstetter-johannes.github.io/publication/medina-2018-rudder/</guid><description/></item></channel></rss>